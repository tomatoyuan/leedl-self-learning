{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ed01c5",
   "metadata": {},
   "source": [
    "# 必要语法解释\n",
    "\n",
    "补充在完成作业过程中涉及到的语法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4c074",
   "metadata": {},
   "source": [
    "## 向量、矩阵、张量\n",
    "\n",
    "- 张量 (Tensor) 和向量 (Vector) 的区别\n",
    "    - 向量 (Vector)：\n",
    "    - 向量是一维数组，用于表示空间中的方向和大小。\n",
    "    - 向量是数学概念，通常指具有大小和方向的量。\n",
    "    - 在机器学习中，向量通常指一维张量。\n",
    "\n",
    "- 张量 (Tensor)：\n",
    "    - 张量是向量和矩阵的推广，可以是任意维度的数组。\n",
    "    - 零维张量是标量，一维张量是向量，二维张量是矩阵，三维及以上称为多维张量。\n",
    "    - 在深度学习中，张量是最基本的数据结构，用于存储和处理数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe61f64",
   "metadata": {},
   "source": [
    "\n",
    "## repeat函数\n",
    "在 PyTorch 中，repeat() 函数用于复制张量中的元素，沿指定维度扩展张量的形状。\n",
    "```python\n",
    "tensor.repeat(*sizes)\n",
    "```\n",
    "\n",
    "- 参数：*sizes 是一个或多个整数，指定每个维度上的重复次数。\n",
    "- 返回：返回一个新的张量，其元素是原张量的重复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa60995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 1, 2, 3, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1维张量重复\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x = x.repeat(3) # 沿着第0维重复三次\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40c7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4]])\n",
      "tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4],\n",
      "        [1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 2维张量重复\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]]) # 从外往里，维度依次升高\n",
    "print(x.repeat(2, 1)) # 第0维重复2次\n",
    "print(x.repeat(1, 3)) # 第1维重复3次\n",
    "x = x.repeat(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fafea9",
   "metadata": {},
   "source": [
    "## unsqueeze 函数\n",
    "在 PyTorch 中，unsqueeze() 函数用于在张量的指定位置插入一个维度，其值为 1。这在需要调整张量形状以匹配特定操作（如广播）时非常有用。\n",
    "\n",
    "```python\n",
    "tensor.unsqueeze(dim)\n",
    "```\n",
    "\n",
    "- 参数：dim 是要插入新维度的位置（整数）。\n",
    "- 返回：返回一个新的张量，其维度比原张量多 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1077077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第0维插入维度：\n",
      "tensor([[1, 2, 3]]) torch.Size([1, 3])\n",
      "在第1维插入维度：\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 一维张量 ➡️ 二维张量\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3]) # 在第几维插入维度，就将第几维的所有元素用列表[]包起来\n",
    "\n",
    "print(\"在第0维插入维度：\")\n",
    "x1 = x.unsqueeze(0)\n",
    "print(x1, x1.size())\n",
    "\n",
    "print(\"在第1维插入维度：\")\n",
    "x2 = x.unsqueeze(1)\n",
    "print(x2, x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984b99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]]]) torch.Size([1, 2, 2])\n",
      "tensor([[[1, 2]],\n",
      "\n",
      "        [[3, 4]]]) torch.Size([2, 1, 2])\n",
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]]]) torch.Size([2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 二维张量 ➡️ 三维张量\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(x.shape)\n",
    "\n",
    "x1 = x.unsqueeze(0)\n",
    "print(x1, x1.size())\n",
    "\n",
    "x2 = x.unsqueeze(1)\n",
    "print(x2, x2.size())\n",
    "\n",
    "x3 = x.unsqueeze(2)\n",
    "print(x3, x3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32f1c3",
   "metadata": {},
   "source": [
    "## squeeze 函数\n",
    "\n",
    "与unsqueeze函数相反，该函数是删减特定维度。删除第几维，就去掉第几维的[]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35665e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5],\n",
      "        [7],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])     # 形状: [3]\n",
    "b = torch.tensor([[4], [5], [6]])    # 形状: [2, 1]\n",
    "\n",
    "# 错误：无法直接相加\n",
    "# c = a + b\n",
    "\n",
    "# 正确：调整 a 的形状为 [3, 1]\n",
    "c = a.unsqueeze(1) + b\n",
    "print(c)\n",
    "# 输出:\n",
    "# tensor([[5, 6, 7],\n",
    "#         [6, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3bbad",
   "metadata": {},
   "source": [
    "## 广播\n",
    "\n",
    "在 PyTorch 中，广播（Broadcasting） 是一种强大的机制，允许不同形状的张量进行算术运算，而无需显式复制数据。\n",
    "\n",
    "**广播的核心规则**\n",
    "\n",
    "- 维度对齐：从最后一个维度开始向前比较，维度大小必须：\n",
    "    - 相等，或\n",
    "    - 其中一个为 1。\n",
    "- 维度扩展：若某个张量的维度更少，则在左侧补 1，直到维度数匹配。\n",
    "- 元素操作：每个维度上的操作会自动扩展到所有元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6772e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 12, 13],\n",
      "        [21, 22, 23]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1. 向量与标量相加\n",
    "# 2. 向量与矩阵相加\n",
    "a = torch.tensor([1, 2, 3])     # 形状：[3]\n",
    "b = torch.tensor([[10], [20]])  # 形状：[2, 1]\n",
    "\n",
    "# 调整 a 的维度，在最前面加一维\n",
    "# a.shape=(1, 3), b.shape=(2, 1)\n",
    "# 广播后的形状：[2, 3]，每个维度将1维的值加到另一个的所有维度上\n",
    "c = a.unsqueeze(0) + b\n",
    "print(c, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98af5eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3082, 0.4318, 0.7938, 0.9368],\n",
      "         [0.4784, 0.9879, 0.7584, 0.7286]],\n",
      "\n",
      "        [[1.0045, 0.2503, 0.9643, 1.0838],\n",
      "         [1.1747, 0.8065, 0.9289, 0.8756]],\n",
      "\n",
      "        [[1.0037, 0.4848, 0.9543, 0.5802],\n",
      "         [1.1739, 1.0409, 0.9189, 0.3720]]]) torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3. 三维张量的广播\n",
    "a = torch.rand(3, 1, 4) # shape: (3, 1, 4)\n",
    "b = torch.rand(1, 2, 4) # shape: (1, 2, 4)\n",
    "\n",
    "c = a + b\n",
    "print(c, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc2e24",
   "metadata": {},
   "source": [
    "## expend 函数\n",
    "\n",
    "在 PyTorch 中，expand() 用于扩展张量的形状，使其与目标形状匹配。与 repeat() 不同，expand() 不会复制实际数据，而是通过广播机制在逻辑上扩展张量。\n",
    "\n",
    "```python\n",
    "tensor.expand(*sizes)\n",
    "```\n",
    "\n",
    "- \\*sizes：目标形状的元组，表示每个维度需要扩展的大小。\n",
    "- 返回一个逻辑上扩展后的新张量，数据并未实际复制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c413dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 一维张量扩展\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3]) # shape=(3)\n",
    "x1 = x.expand(2, 3)         # 不进行数据复制操作，相当于进行广播: ？+ (3) = (2, 3) ➡️ (2, 3或1) + (3).unsqueeze(0) = (2, 3)，相当于对一个空的张量进行了广播操作\n",
    "\n",
    "print(x1, x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e37a4df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1],\n",
      "        [2, 2, 2, 2],\n",
      "        [3, 3, 3, 3]]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 二维张量扩展\n",
    "x = torch.tensor([[1], [2], [3]]) # shape:(3, 1)\n",
    "y = x.expand(3, 4)  # 扩展为 shape=(3, 4) 利用广播的原理扩展第1维\n",
    "\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e0c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 使用 -1 保持原维度\n",
    "x = torch.rand(2, 1, 3)\n",
    "x1 = x.expand(2, 1, -1) # 第2维保持不变\n",
    "\n",
    "print(x.shape)\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e5fd6",
   "metadata": {},
   "source": [
    "## expand_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a5a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 1)  # 形状: [3, 1]\n",
    "b = torch.rand(3, 4)  # 形状: [3, 4]\n",
    "\n",
    "# 扩展 a 以匹配 b 的形状\n",
    "a_expanded = a.expand_as(b)  # 等价于 a.expand(3, 4)\n",
    "print(a_expanded.shape)\n",
    "c = a_expanded + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451890b",
   "metadata": {},
   "source": [
    "## torch.empty()\n",
    "\n",
    "在 PyTorch 中，torch.empty() 用于创建未初始化的张量（即张量的值是内存中已有的随机值，未被显式初始化）。这使得它比 torch.zeros() 或 torch.ones() 更快，因为不需要填充特定值。\n",
    "\n",
    "```python\n",
    "torch.empty(*size, dtype=None, device=None, requires_grad=False)\n",
    "```\n",
    "\n",
    "参数：\n",
    "- *size：张量的形状（如 (3, 4) 表示 3×4 的矩阵）。\n",
    "- dtype：数据类型（如 torch.float32，默认）。\n",
    "- device：存储设备（如 'cpu' 或 'cuda'）。\n",
    "- requires_grad：是否跟踪梯度（True 或 False）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936427a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建未初始化的二维张量\n",
    "import torch\n",
    "\n",
    "x = torch.empty(3, 4)\n",
    "print(x)\n",
    "# 本应该是随机数，但新买的电脑就是干净，全是0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e80b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 2. 指定数据类型和设备\n",
    "# 创建一个在 GPU 上的 int64 类型张量\n",
    "x = torch.empty(2, 3, dtype=torch.int64, device='cpu') # 因为写代码用了mac，只有cpu，😆\n",
    "print(x)\n",
    "# 输出（随机值）：\n",
    "# tensor([[0, 0, 0],\n",
    "#         [0, 0, 0]], device='cuda:0', dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365b897",
   "metadata": {},
   "source": [
    "## tqdm\n",
    "\n",
    "tqdm 可以接受多个参数来自定义进度条的显示，常用参数如下：\n",
    "- desc：进度条的前缀说明文字\n",
    "- total：指定迭代的总次数\n",
    "- ncols：指定进度条的宽度\n",
    "- unit：指定进度条的单位\n",
    "- colour：指定进度条的颜色\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d138b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|\u001b[32m          \u001b[0m| 0/3 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  33%|\u001b[32m███▎      \u001b[0m| 1/3 [00:01<00:02,  1.01s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  67%|\u001b[32m██████▋   \u001b[0m| 2/3 [00:02<00:01,  1.01s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|\u001b[32m██████████\u001b[0m| 3/3 [00:03<00:00,  1.01s/file]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "temp_list = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "for i, fname in tqdm(enumerate(temp_list), desc=\"Processing files\", total=len(temp_list), unit=\"file\", colour='green'):\n",
    "    print(fname)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bed53f",
   "metadata": {},
   "source": [
    "## tqdm 与 enumerate 嵌套顺序\n",
    "\n",
    "在Python中，`tqdm`（进度条库）和`enumerate`（获取索引和值）的嵌套顺序会影响进度条的显示方式和索引值的计算。具体来说：\n",
    "\n",
    "### **关键区别**\n",
    "| 顺序               | 进度条总数         | 索引 `i` 的含义          | 示例输出                     |\n",
    "|--------------------|--------------------|-------------------------|------------------------------|\n",
    "| `tqdm(enumerate())` | 与数据长度一致     | 数据的真实索引           | `100%|██████████| 4/4 [00:00]` |\n",
    "| `enumerate(tqdm())` | 与数据长度一致     | 循环计数（与进度条无关） | `100%|██████████| 4/4 [00:00]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **总结**\n",
    "1. **推荐顺序**：`tqdm(enumerate(data))`  \n",
    "   - 同时获得真实索引和正确的进度条显示。\n",
    "\n",
    "2. **特殊场景**：`enumerate(tqdm(data))`  \n",
    "   - 仅需进度条，不依赖索引与进度的关联（如嵌套循环中的内层进度）。\n",
    "\n",
    "3. **进度条参数**：  \n",
    "   - 使用 `total=len(data)` 确保进度条总数正确。\n",
    "   - 使用 `leave=False` 避免多层进度条残留。\n",
    "\n",
    "> 经实践验证，两种情况跑出来的效果完全一致，damn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e69a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 0, 值: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 1, 值: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:01<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 2, 值: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 3, 值: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# 场景 1：tqdm(enumerate())\n",
    "data = [10, 20, 30, 40]\n",
    "for i, x in tqdm(enumerate(data), total=len(data)):\n",
    "    print(f\"索引: {i}, 值: {x}\")\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbac197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 0, 值: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:00<00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 1, 值: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:01<00:01,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 2, 值: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引: 3, 值: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# 场景 2：enumerate(tqdm())\n",
    "data = [10, 20, 30, 40]\n",
    "for i, x in enumerate(tqdm(data)):\n",
    "    print(f\"索引: {i}, 值: {x}\")\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b5f8b",
   "metadata": {},
   "source": [
    "## torch.max()\n",
    "\n",
    "在 PyTorch 中，`torch.max()` 函数主要用于返回输入张量中的最大值或指定维度上的最大值。它有两种常见用法：\n",
    "\n",
    "\n",
    "### **1. 返回整个张量的最大值**\n",
    "#### **语法**\n",
    "```python\n",
    "torch.max(input) → scalar\n",
    "```\n",
    "- **参数**：`input` 是要处理的张量。\n",
    "- **返回**：张量中的最大值（标量）。\n",
    "\n",
    "#### **示例**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "max_value = torch.max(x)\n",
    "print(max_value)  # 输出: tensor(6)\n",
    "```\n",
    "\n",
    "\n",
    "### **2. 返回指定维度上的最大值及索引**\n",
    "#### **语法**\n",
    "```python\n",
    "torch.max(input, dim, keepdim=False) → (values, indices)\n",
    "```\n",
    "- **参数**：\n",
    "  - `input`：输入张量。\n",
    "  - `dim`：指定要缩减的维度（如 `0` 表示按列，`1` 表示按行）。\n",
    "  - `keepdim`：是否保持输出的维度与输入一致（默认 `False`）。\n",
    "- **返回**：一个元组 `(values, indices)`，其中：\n",
    "  - `values`：指定维度上的最大值。\n",
    "  - `indices`：最大值在原张量中的索引位置。\n",
    "\n",
    "#### **示例**\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "\n",
    "# 按列（dim=0）计算最大值\n",
    "max_values, indices = torch.max(x, dim=0)\n",
    "print(\"最大值:\", max_values)  # 输出: tensor([4, 6, 5])\n",
    "print(\"索引:\", indices)      # 输出: tensor([1, 1, 1])\n",
    "\n",
    "# 按行（dim=1）计算最大值\n",
    "max_values, indices = torch.max(x, dim=1)\n",
    "print(\"最大值:\", max_values)  # 输出: tensor([3, 6])\n",
    "print(\"索引:\", indices)      # 输出: tensor([1, 1])\n",
    "```\n",
    "\n",
    "\n",
    "### **3. 结合 `keepdim=True` 使用**\n",
    "当 `keepdim=True` 时，输出的维度与输入相同，只是指定维度的大小变为 1。\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "\n",
    "max_values, indices = torch.max(x, dim=1, keepdim=True)\n",
    "print(\"最大值形状:\", max_values.shape)  # 输出: torch.Size([2, 1])\n",
    "print(\"最大值:\", max_values)\n",
    "# 输出:\n",
    "# tensor([[3],\n",
    "#         [6]])\n",
    "```\n",
    "\n",
    "\n",
    "### **4. 在实际代码中的应用**\n",
    "#### **示例：计算 softmax 后的预测类别**\n",
    "```python\n",
    "logits = torch.tensor([[1.2, 3.4, 0.8], [2.1, 0.5, 1.8]])\n",
    "probs = torch.softmax(logits, dim=1)  # 计算概率分布\n",
    "\n",
    "# 获取预测类别（概率最大的索引）\n",
    "preds = torch.argmax(probs, dim=1)  # 等价于 torch.max(probs, dim=1)[1]\n",
    "print(\"预测类别:\", preds)  # 输出: tensor([1, 0])\n",
    "```\n",
    "\n",
    "\n",
    "### **5. 与 `torch.argmax()` 的区别**\n",
    "- **`torch.max()`**：返回最大值 **和** 对应的索引。\n",
    "- **`torch.argmax()`**：仅返回最大值的索引，不返回值本身。\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2]])\n",
    "\n",
    "max_value, index = torch.max(x, dim=1)\n",
    "print(\"最大值:\", max_value)  # 输出: tensor([3])\n",
    "print(\"索引:\", index)      # 输出: tensor([1])\n",
    "\n",
    "index = torch.argmax(x, dim=1)\n",
    "print(\"索引:\", index)      # 输出: tensor([1])\n",
    "```\n",
    "\n",
    "\n",
    "### **总结**\n",
    "| 用法                | 示例                     | 输出                                  |\n",
    "|---------------------|--------------------------|---------------------------------------|\n",
    "| 全局最大值          | `torch.max(x)`           | 标量（如 `tensor(6)`）                |\n",
    "| 按维度最大值        | `torch.max(x, dim=0)`    | 元组 `(values, indices)`              |\n",
    "| 仅获取索引          | `torch.argmax(x, dim=1)` | 索引张量（如 `tensor([1, 0])`）       |\n",
    "| 保持维度            | `torch.max(x, dim=1, keepdim=True)` | 保持原维度的最大值张量 |\n",
    "\n",
    "`torch.max()` 是深度学习中常用的函数，尤其在分类任务中用于获取预测类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070fce85",
   "metadata": {},
   "source": [
    "## Tensor.detach()\n",
    "\n",
    "在 PyTorch 中，`torch.max()` 函数主要用于返回输入张量中的最大值或指定维度上的最大值。它有两种常见用法：\n",
    "\n",
    "\n",
    "### **1. 返回整个张量的最大值**\n",
    "#### **语法**\n",
    "```python\n",
    "torch.max(input) → scalar\n",
    "```\n",
    "- **参数**：`input` 是要处理的张量。\n",
    "- **返回**：张量中的最大值（标量）。\n",
    "\n",
    "#### **示例**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "max_value = torch.max(x)\n",
    "print(max_value)  # 输出: tensor(6)\n",
    "```\n",
    "\n",
    "\n",
    "### **2. 返回指定维度上的最大值及索引**\n",
    "#### **语法**\n",
    "```python\n",
    "torch.max(input, dim, keepdim=False) → (values, indices)\n",
    "```\n",
    "- **参数**：\n",
    "  - `input`：输入张量。\n",
    "  - `dim`：指定要缩减的维度（如 `0` 表示按列，`1` 表示按行）。\n",
    "  - `keepdim`：是否保持输出的维度与输入一致（默认 `False`）。\n",
    "- **返回**：一个元组 `(values, indices)`，其中：\n",
    "  - `values`：指定维度上的最大值。\n",
    "  - `indices`：最大值在原张量中的索引位置。\n",
    "\n",
    "#### **示例**\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "\n",
    "# 按列（dim=0）计算最大值\n",
    "max_values, indices = torch.max(x, dim=0)\n",
    "print(\"最大值:\", max_values)  # 输出: tensor([4, 6, 5])\n",
    "print(\"索引:\", indices)      # 输出: tensor([1, 1, 1])\n",
    "\n",
    "# 按行（dim=1）计算最大值\n",
    "max_values, indices = torch.max(x, dim=1)\n",
    "print(\"最大值:\", max_values)  # 输出: tensor([3, 6])\n",
    "print(\"索引:\", indices)      # 输出: tensor([1, 1])\n",
    "```\n",
    "\n",
    "\n",
    "### **3. 结合 `keepdim=True` 使用**\n",
    "当 `keepdim=True` 时，输出的维度与输入相同，只是指定维度的大小变为 1。\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2], [4, 6, 5]])\n",
    "\n",
    "max_values, indices = torch.max(x, dim=1, keepdim=True)\n",
    "print(\"最大值形状:\", max_values.shape)  # 输出: torch.Size([2, 1])\n",
    "print(\"最大值:\", max_values)\n",
    "# 输出:\n",
    "# tensor([[3],\n",
    "#         [6]])\n",
    "```\n",
    "\n",
    "\n",
    "### **4. 在实际代码中的应用**\n",
    "#### **示例：计算 softmax 后的预测类别**\n",
    "```python\n",
    "logits = torch.tensor([[1.2, 3.4, 0.8], [2.1, 0.5, 1.8]])\n",
    "probs = torch.softmax(logits, dim=1)  # 计算概率分布\n",
    "\n",
    "# 获取预测类别（概率最大的索引）\n",
    "preds = torch.argmax(probs, dim=1)  # 等价于 torch.max(probs, dim=1)[1]\n",
    "print(\"预测类别:\", preds)  # 输出: tensor([1, 0])\n",
    "```\n",
    "\n",
    "\n",
    "### **5. 与 `torch.argmax()` 的区别**\n",
    "- **`torch.max()`**：返回最大值 **和** 对应的索引。\n",
    "- **`torch.argmax()`**：仅返回最大值的索引，不返回值本身。\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1, 3, 2]])\n",
    "\n",
    "max_value, index = torch.max(x, dim=1)\n",
    "print(\"最大值:\", max_value)  # 输出: tensor([3])\n",
    "print(\"索引:\", index)      # 输出: tensor([1])\n",
    "\n",
    "index = torch.argmax(x, dim=1)\n",
    "print(\"索引:\", index)      # 输出: tensor([1])\n",
    "```\n",
    "\n",
    "\n",
    "### **总结**\n",
    "| 用法                | 示例                     | 输出                                  |\n",
    "|---------------------|--------------------------|---------------------------------------|\n",
    "| 全局最大值          | `torch.max(x)`           | 标量（如 `tensor(6)`）                |\n",
    "| 按维度最大值        | `torch.max(x, dim=0)`    | 元组 `(values, indices)`              |\n",
    "| 仅获取索引          | `torch.argmax(x, dim=1)` | 索引张量（如 `tensor([1, 0])`）       |\n",
    "| 保持维度            | `torch.max(x, dim=1, keepdim=True)` | 保持原维度的最大值张量 |\n",
    "\n",
    "`torch.max()` 是深度学习中常用的函数，尤其在分类任务中用于获取预测类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2240fa",
   "metadata": {},
   "source": [
    "## 计算 acc 和 loss 时的细节区别\n",
    "\n",
    "### **为什么 `train_acc/len(train_set)` 和 `train_loss/len(train_loader)` 除以不同的长度？**\n",
    "这与 **`train_set` 和 `train_loader` 的含义**以及 **准确率和损失的计算方式** 有关：\n",
    "\n",
    "#### **1. 数据集长度 vs 数据加载器长度**\n",
    "- **`train_set`**：表示训练集的**总样本数**（例如，1000个样本）。\n",
    "- **`train_loader`**：表示训练集的数据加载器，通常会将数据分成多个**批次**（batch）。假设每个批次有32个样本，则 `len(train_loader)` 为 **总批次数**（例如，1000/32 ≈ 32个批次）。\n",
    "\n",
    "\n",
    "#### **2. 准确率和损失的计算逻辑**\n",
    "- **准确率（Accuracy）**：是 **正确预测的样本数 / 总样本数**。\n",
    "  - `train_acc` 累加的是每个批次中正确预测的样本数（例如，32个样本中预测对了28个）。\n",
    "  - 因此，需要除以 **总样本数 `len(train_set)`** 来得到平均准确率。\n",
    "\n",
    "- **损失（Loss）**：是 **每个批次的平均损失之和**。\n",
    "  - `train_loss` 累加的是每个批次的**平均损失**（即 `loss.item()`，一个批次内所有样本的平均损失）。\n",
    "  - 因此，需要除以 **总批次数 `len(train_loader)`** 来得到所有批次的平均损失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921b18a",
   "metadata": {},
   "source": [
    "## np.concatenate\n",
    "\n",
    "在 Python 的 NumPy 库中，np.concatenate函数用于将多个数组沿指定轴连接成一个新数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bea676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 一维数组的连接\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "result = np.concatenate((a, b), axis=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2cf6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "[[1 2 5 6]\n",
      " [3 4 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# 二维数组的连接\n",
    "import numpy as np\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# 沿第0维拼接\n",
    "result = np.concatenate((a, b), axis=0)\n",
    "print(result)\n",
    "\n",
    "# 沿第1维拼接\n",
    "result2 = np.concatenate((a, b), axis=1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cf977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个数组拼接要注意，连接的维度必须保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd03110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LEEDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
